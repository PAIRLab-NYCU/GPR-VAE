{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "import math\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as KTF\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   # Dynamic allocation\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "#KTF.set_session(sess)\n",
    "path = './GPR_npy/npy_train_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = np.load(path+'/RSSI_train.npy', allow_pickle=True)\n",
    "AP = len(real_data[0])\n",
    "AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(reconstruction, x,rssi_gpr, mu, log_var):\n",
    "    x_temp = tf.reshape(x, [-1, AP])\n",
    "#     q = np.quantile(x_temp.numpy(),0.05)\n",
    "    \n",
    "    x_idx = x_temp != -100\n",
    "    x_idx_h = x_temp >= -55\n",
    "    \n",
    "    gt = x_temp.numpy().copy()\n",
    "    gt[gt <= -100] = np.nan\n",
    "    mean_gt = np.nanmean(gt, axis=1)\n",
    "#     max_gt_idx = np.nanmax(gt, axis=1) > -60\n",
    "    var_gt = np.nanvar(gt, axis=1)\n",
    "    \n",
    "    rec = reconstruction.numpy().copy()\n",
    "    rec[rec <= -100] = np.nan\n",
    "    mean_rec = np.nanmean(rec, axis=1)\n",
    "    var_rec = np.nanvar(rec, axis=1)\n",
    "    \n",
    "    mv = []\n",
    "    mean_loss = tf.reduce_sum(abs(mean_gt-mean_rec))\n",
    "    var_loss = tf.reduce_sum(abs(var_gt-var_rec))\n",
    "    mv.append(tf.reduce_sum(mean_loss + var_loss))\n",
    "    mv = tf.reduce_sum(mv)\n",
    "    \n",
    "#     mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "#     mse_loss = mse(x_temp[x_idx], reconstruction[x_idx])\n",
    "#     reconstruction_loss = tf.reduce_sum(mse_loss)\n",
    "    \n",
    "    l1 = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    l1_loss = l1(x_temp[x_idx], reconstruction[x_idx]) \n",
    "    reconstruction_loss_l1 = tf.reduce_sum(l1_loss)\n",
    "    \n",
    "    l1 = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    l1_loss = l1(x_temp, reconstruction) \n",
    "    l1_loss = abs(x_temp - 100) * abs(x_temp - 100) * l1_loss\n",
    "    reconstruction_loss_l1_100 = tf.reduce_sum(l1_loss)\n",
    "    \n",
    "    l1 = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    x_idx_max = x_temp == np.nanmax(x_temp)\n",
    "    x_idx_min = x_temp == np.nanmin(x_temp)    \n",
    "    l1_loss_max = l1(x_temp[x_idx_max], reconstruction[x_idx_max])/np.sum(x_idx_max)\n",
    "    l1_loss_min = l1(x_temp[x_idx_min], reconstruction[x_idx_min])/np.sum(x_idx_min)\n",
    "#     print(np.sum(x_idx_max), np.sum(x_idx_min))\n",
    "    reconstruction_loss_l1_mm = l1_loss_max + l1_loss_min\n",
    "    \n",
    "    \n",
    "    \n",
    "    l1 = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    l1_loss_h = l1(x_temp[x_idx_h], reconstruction[x_idx_h])\n",
    "    reconstruction_loss_l1_h = tf.reduce_sum(l1_loss_h)\n",
    "    \n",
    "    kl_loss = -0.5 * (1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
    "    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "    \n",
    "    li = [reconstruction_loss_l1_100 * 1.2/10000000 , reconstruction_loss_l1_mm * 100, mv * 0.01, kl_loss]\n",
    "\n",
    "#     flag = 0\n",
    "    total_loss = reconstruction_loss_l1\n",
    "    for i in li:\n",
    "        if not math.isnan(i):\n",
    "            if i != 0:\n",
    "                total_loss += i\n",
    "    return total_loss,reconstruction_loss_l1,reconstruction_loss_l1_h,mean_loss,var_loss,kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self,z_dim,initializer,**kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.leakyReLU = layers.LeakyReLU()\n",
    "        self.ReLU = layers.ReLU()\n",
    "#         self.dropout = tf.keras.layers.Dropout(.05)\n",
    "        \n",
    "        self.f1 = layers.Dense(AP*3,input_shape=(AP,),kernel_initializer=initializer)\n",
    "        self.f2 = layers.Dense(int(AP*2.8),kernel_initializer=initializer)\n",
    "        self.f3 = layers.Dense(int(AP*2.5),input_shape=(AP,),kernel_initializer=initializer)\n",
    "        self.f4 = layers.Dense(int(AP*2),kernel_initializer=initializer)\n",
    "        self.fc11 = layers.Dense(int(AP*1.8),kernel_initializer=initializer)\n",
    "        self.fc12 = layers.Dense(z_dim,kernel_initializer=initializer)\n",
    "\n",
    "        self.fc21 = layers.Dense(int(AP*1.8),kernel_initializer=initializer)\n",
    "        self.fc22 = layers.Dense(z_dim,kernel_initializer=initializer)\n",
    "\n",
    "        # For decoder\n",
    "        self.fc5 = layers.Dense(int(AP*2.5),kernel_initializer=initializer)\n",
    "        self.fc6 = layers.Dense(int(AP*2),kernel_initializer=initializer)\n",
    "        self.fc7 = layers.Dense(int(AP*1.8),kernel_initializer=initializer)\n",
    "        self.fc8 = layers.Dense(int(AP*1.5),kernel_initializer=initializer)\n",
    "        self.fc9 = layers.Dense(AP,kernel_initializer=initializer)\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        x = tf.reshape(x, [-1,AP])\n",
    "        h = self.leakyReLU(self.f1(x))\n",
    "#         h = self.dropout(h)\n",
    "        h = self.leakyReLU(self.f2(h))\n",
    "        h = self.leakyReLU(self.f3(h))\n",
    "        h = self.f4(h)\n",
    "        mu = self.leakyReLU(self.fc11(h))\n",
    "        mu = self.fc12(mu)\n",
    "\n",
    "        log_var = self.leakyReLU(self.fc21(h))\n",
    "        log_var = self.fc22(log_var)\n",
    "        return mu, log_var  # mu, log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        h = self.leakyReLU(self.fc5(z))\n",
    "        h = self.leakyReLU(self.fc6(h))\n",
    "        h = self.leakyReLU(self.fc7(h))\n",
    "        h = self.leakyReLU(self.fc8(h))\n",
    "        h = self.fc9(h)\n",
    "        return h \n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        batch = tf.shape(mu)[0]\n",
    "        dim = tf.shape(mu)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return mu + tf.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "    def call(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設置 train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = np.load(path+'/pos_train.npy', allow_pickle=True)\n",
    "real_data = np.load(path+'/RSSI_train.npy', allow_pickle=True)\n",
    "inter_data = np.load(path+'/RSSI_pred_train.npy', allow_pickle=True)\n",
    "BUFFER_SIZE = real_data.shape[0]\n",
    "# keep_threshold = 1\n",
    "\n",
    "def train_generator():\n",
    "    li = []\n",
    "    length = len(real_data)\n",
    "    for index in range(length):\n",
    "        real_fl_array = np.array(real_data[index], dtype=np.float32).reshape(AP,1)\n",
    "        inter_fl_array = np.array(inter_data[index], dtype=np.float32).reshape(AP,1)\n",
    "        pos_f1_array = np.array(pos_data[index], dtype=np.float32)\n",
    "        yield inter_fl_array, real_fl_array, pos_f1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    train_generator,\n",
    "    output_shapes=((AP,1),(AP,1),(2,)), \n",
    "    output_types=(tf.float32,tf.float32,tf.float32))\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE).shuffle(batch_size).repeat(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設置訓練函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(rssi_gpr, rssi_gt, pos_gt,train=False):\n",
    "    \n",
    "    with tf.GradientTape() as vae_tape, tf.GradientTape() as pos_tape:\n",
    "        reconstruction, mu, log_var, z = vae(rssi_gpr, training=train)\n",
    "        \n",
    "        vae_loss, vae_l1_loss, vae_l1_loss_h,mean_loss,var_loss,kl_loss = vae_loss_function(reconstruction,rssi_gt,rssi_gpr, mu, log_var)\n",
    "    if not math.isnan(vae_loss):\n",
    "        gradients_of_vae = vae_tape.gradient(vae_loss, vae.trainable_variables)\n",
    "        vae_optimizer.apply_gradients(zip(gradients_of_vae, vae.trainable_variables))\n",
    "\n",
    "    return vae_loss, vae_l1_loss, vae_l1_loss_h,mean_loss,var_loss,kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(start_ep,epochs):\n",
    "    min_l1 = 999\n",
    "    skip = 0\n",
    "    flag = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(start_ep,epochs):\n",
    "        vae_loss_li = []\n",
    "        pos_loss_li = []\n",
    "        vae_loss_li_l1 = []\n",
    "        mloss_li = []\n",
    "        vloss_li = []\n",
    "        kl_li = []\n",
    "#         mean_loss,var_loss\n",
    "        for rssi_gpr, rssi_gt, pos_gt in train_ds:\n",
    "            \n",
    "#             print(rssi_gpr[0][0],rssi_gt[0][0],pos_gt[0][0])\n",
    "#             raise\n",
    "            vae_loss, vae_l1_loss,vae_l1_loss_h,mloss,vloss,kl_loss = train_step(rssi_gpr, rssi_gt, pos_gt,True)\n",
    "            vloss_li.append(vloss.numpy())\n",
    "            mloss_li.append(mloss.numpy())\n",
    "            vae_loss_li.append(vae_loss.numpy())\n",
    "            vae_loss_li_l1.append(vae_l1_loss.numpy())\n",
    "            pos_loss_li.append(vae_l1_loss_h.numpy())\n",
    "            kl_li.append(kl_loss.numpy())\n",
    "#             pos_loss_li.append(pos_loss.numpy())\n",
    "        tf.print(f'{time.time() - start_time: 06.0f}s Epoch: {epoch+1} vae_loss: {np.nanmean(vae_loss_li):.2f} vae_loss_l1: {np.nanmean(vae_loss_li_l1):.2f} vae_loss_l1_h: {np.nanmean(pos_loss_li):.2f} mloss: {np.nanmean(mloss_li):.2f} vloss: {np.nanmean(vloss_li):.2f}', end = ' ')\n",
    "        tf.print(f'kl_loss {np.mean(kl_li):.2f}')\n",
    "#         print()\n",
    "        if np.nanmean(vae_loss_li) <= 80 and flag == 0:\n",
    "            print('lr:1e-5')\n",
    "            vae_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "            flag = 1\n",
    "            \n",
    "        if np.nanmean(vae_loss_li) <= 50 and flag == 1:\n",
    "            print('lr:1e-6')\n",
    "            vae_optimizer = tf.keras.optimizers.Adam(1e-6)\n",
    "            flag = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_decayed_fn = tf.compat.v1.train.cosine_decay(1e-3, 1000,1)\n",
    "\n",
    "z_dim = int(AP*0.8)\n",
    "initializer = tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.01, seed=None\n",
    ")\n",
    "\n",
    "vae = VAE(z_dim,initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 00005s Epoch: 1 vae_loss: 11248.05 vae_loss_l1: 81.40 vae_loss_l1_h: 42.14 mloss: 7006.24 vloss: 21630.38 kl_loss 0.00\n",
      " 00009s Epoch: 2 vae_loss: 11247.54 vae_loss_l1: 81.39 vae_loss_l1_h: 42.14 mloss: 7005.93 vloss: 21630.38 kl_loss 0.00\n",
      " 00015s Epoch: 3 vae_loss: 11244.26 vae_loss_l1: 81.37 vae_loss_l1_h: 42.11 mloss: 7003.87 vloss: 21630.38 kl_loss 0.10\n",
      " 00019s Epoch: 4 vae_loss: 11058.37 vae_loss_l1: 78.81 vae_loss_l1_h: 39.68 mloss: 6760.76 vloss: 21608.08 kl_loss 149.95\n",
      " 00022s Epoch: 5 vae_loss: 6427.66 vae_loss_l1: 31.58 vae_loss_l1_h: 18.03 mloss: 2445.08 vloss: 16483.69 kl_loss 910.87\n",
      " 00025s Epoch: 6 vae_loss: 3257.25 vae_loss_l1: 16.03 vae_loss_l1_h: 21.19 mloss: 847.20 vloss: 14776.08 kl_loss 259.68\n",
      " 00028s Epoch: 7 vae_loss: 2925.29 vae_loss_l1: 20.68 vae_loss_l1_h: 20.25 mloss: 1217.12 vloss: 22160.94 kl_loss 191.59\n",
      " 00030s Epoch: 8 vae_loss: 2216.92 vae_loss_l1: 21.08 vae_loss_l1_h: 21.36 mloss: 1489.70 vloss: 37912.77 kl_loss 208.85\n",
      " 00032s Epoch: 9 vae_loss: 1791.26 vae_loss_l1: 17.24 vae_loss_l1_h: 18.96 mloss: 1189.35 vloss: 28875.18 kl_loss 205.12\n",
      " 00036s Epoch: 10 vae_loss: 2054.04 vae_loss_l1: 18.68 vae_loss_l1_h: 18.19 mloss: 1113.79 vloss: 26255.22 kl_loss 195.06\n"
     ]
    }
   ],
   "source": [
    "# min_l1,min_l1_ep = \n",
    "trainer(0, 10) #NO標準化 D_5G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設置 expand dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_inter_data = np.load(path+'/RSSI_pred_grid.npy', allow_pickle=True)\n",
    "ext_pos_data = np.load(path+'/grid_POS.npy', allow_pickle=True)\n",
    "ext_real_data = np.load(path+'/RSSI_pred_grid.npy', allow_pickle=True)\n",
    "\n",
    "def grid_generator():\n",
    "    li = []\n",
    "    length_t = len(ext_inter_data)\n",
    "    for index in range(length_t):\n",
    "        real_array = np.array(ext_real_data[index], dtype=np.float32).reshape(AP,1)\n",
    "        inter_array = np.array(ext_inter_data[index], dtype=np.float32).reshape(AP,1)\n",
    "        pos_array = np.array(ext_pos_data[index], dtype=np.float32)\n",
    "        \n",
    "        yield inter_array, real_array, pos_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生外擴資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "grid_ds = tf.data.Dataset.from_generator(\n",
    "    grid_generator, \n",
    "    output_shapes=((AP,1),(AP,1),(2,)), \n",
    "    output_types=(tf.float32,tf.float32,tf.float32))\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "grid_ds = grid_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)#.repeat(10)#.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(rssi_gpr, rssi_gt, pos_gt) in enumerate(grid_ds):\n",
    "    reconstruction, mu, log_var, z = vae(rssi_gpr)\n",
    "    if i == 0:\n",
    "        train_pre = reconstruction.numpy()\n",
    "    else:\n",
    "        train_pre = np.concatenate((train_pre,reconstruction.numpy()),axis = 0)\n",
    "\n",
    "np.save(os.path.join(path, 'pre_grid.npy'), train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 158), './GPR_npy/npy_train_data')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pre.shape,path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 儲存 model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./GPR_npy/npy_train_data\n"
     ]
    }
   ],
   "source": [
    "print(path)\n",
    "vae.save_weights('./mean_var_b3_AP'+str(AP)+'_'+str(z_dim)+'/savepoint', save_format='tf')\n",
    "# test.load_weights('test_b3_drop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
